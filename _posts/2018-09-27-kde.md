---
layout: post
title: "Kernel Density Estimation"
excerpt: "KNN"
categories: articles
tags: [machine learning]
comments: true
share: true
---

# Kernel Density Estimation

We want a non-parametric method to estimate a distribution given a dataset. Histogram can be seen as a simple way to do kernel
density estimation. To formulate the problem more genearlly, for a given vector $$x$$, drawn from a distribution $$p(x)$$, will
fall into a given region $$R$$ with the probability of: $$P = \int_R p(x') dx'$$. Let's suppose that there are $$N$$ vectors 
drawn from the distribution, the probability of k vectors within N vectors falls into the region is $$P(k) = N \choose k P^k
(1-P)^(N-k)$$. From the property of binomial distribution, we know that we can estimate $$P$$ as $$P = \frac{k}{N}$$. On the 
other hand, we can estimate $$P$$ if the region is very small, by $$P = p(x) * V$$, where $$V$$ is the volume enclosed by region
R. Therefore, we would have such formualt $$p(x) = \frac{k}{NV}$$, where V is volume surrounding x, N is # of total samples, k 
is # of samples inside V.

# Parzen window

To find how many samples inside V, we can define a kernel function $$K(u) = 1 if |u_j| \le \frac{1}{2}, 0 otherwise$$. This kernel
is known as Parzen window. The quantity $$K(\frac{x - x_n}}{h})$$ equals to one if $$x_n$$ is inside V and 0 otherwise. Therefore,
$$p_{kde}(x) = \frac{1}{Nh^D} \sum_{n=1}^N K(\frac{x - x_n}{h})$$. By looking at the formula, we know that the expectation is 
$$E[p_{kde}(x)] = \frac{1}{Nh^D} \sum_{n=1}^N E[K(\frac{x - x_n}{h})] = \frac{1}{h^D} E[K(\frac{x - x_n}{h})] = \frac{1}{h^d} 
\int K(\frac{x - x_n}{h})p(x) dx$$. We can see that the expecation of $$p_{kde}(x)$$ is a convolution of the true density 
$$p(x)$$ with the kernel function. Wider h is, the smoother the estimate p_{kde}(x) is.  


We can measure the MSE at the esimation point x, given by  
$$E[(p_{kde}(x) - p(x))^2] = E[p_{kde}(x) - p(x)]^2 + var(p_{kde}(x)) = bias^2 + variance$$.  

# Bandwidth selection

To select the bandwidth, we may use subjective methods, plot the density function. But when we are dealing with high-dimensional
data, it's better to use $$h_{MISE} = argmin{E[\int (p_{kde}(x)- p(x))^2 dx]}$$ If we assume the true distribution is Gaussian 
and we use a Gaussian kernel, it can be shown that the optimal value of h is $$ h = 1.06 \sigma N^{\frac{-1}{5}}$$. We can also
use a cross-validation approach to compute the best bandwidth, which is just leave one out and maximize the pseudo-likelihood










